dataset:
    dataset_name: 'f30k'
    root: '<path to pickle files>'
    train_pickle: 'training_set_ltd_l.pickle'
    val_pickle: 'validation_set_ltd_l.pickle'
    test_pickle: 'test_set_ltd_l.pickle'
    vocab_file: 'f30k_vocab.pkl'
    captions_per_image: 5

experiment:
    wandb_project: 'tmlr_experiments'
    experiment_name: 'f30k_ltd_finetune'
    wandb_dir: '/output/f30k/tmlr_experiments'
    out_dir:   '/output/f30k/tmlr_experiments'
    cache_dir: '/output/wenb'

dataloader:
    batch_size: 128
    eval_batch_size: 8
    num_workers: 10
    crop_size: 224
    random_erasing_prob: 0.2
    caption_drop_prob: 0.1
    target_key: 'target-large'

model:
    name: ltd
    embed_dim: 1024
    caption_encoder:
        wemb_type: glove
        word_dim: 300
        tune_from_start: False
        txt_finetune: True
    image_encoder:
         img_finetune: True
         cnn_type: resnet50
         tune_from_start: False
    target_decoder:
        decode_target: False
        reconstruction_dim: 384
        hidden_features: 1024
        input_decoding: False


# optimizer configuration
optimizer:
    name: adamp
    learning_rate: 0.0002
    weight_decay: 0.0
    weight_averaging:
        use_weight_averaging: True
        checkpoints: 5
        percentage: 0.9

# lr scheduler configuration
lr_scheduler:
    name: cosine_annealing
    T_max: 30
    milestones: [15]

# criterion configuration
criterion:
    name: 'infonce'
    tau: 0.05
    margin: 0.2
    reconstruction_metric: 'cosine'

reconstruction_constraint:
    use_constraint: False
    alpha: 0.90
    bound:  0.2
    start_val: 1.
    max: 100.

# detailed training configuration
train:
    model_save_path: model_last.pth
    best_model_save_path: model_best.pth
    n_epochs: 60
    finetune_lr_decay: 0.1
    log_step: 100
    grad_clip: 2
    val_epochs: 1
    use_fp16: True